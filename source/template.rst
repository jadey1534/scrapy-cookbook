==========================================
阿里云-光联SD-WAN项目交付故障演练报告
==========================================

基本拓扑分析
============

|image0|\ 网络拓扑图
--------------------

|image1|\ 北京站点：配备一台PC，PC与CE之间通过wifi连接，获得的地址为10.0.20.101/24，网关为10.0.20.1/24。

杭州站点：配备一台PC，PC与CE之间通过wfi连接，获得的地址为10.0.19.128/24，网关为10.0.19.1/24。

两个站点之间通过BGP路由宣告，实现了10.0.20.0/24网段与10.0.19.0/24网段的互访。

故障演练（2022/4/11）
=====================

 VPNGW故障——主IPSecgw中断 
--------------------------

   **演练目的：验证当主IPSec连接故障，流量切换情况**

   **预期结果：流量默认走主线路，当主线路down掉后，流量会切换到备线，过程中会短时间地出现丢包情况。**

   **演练方式：在北京站点PC（10.0.20.101）与杭州点地wifi网关（10.0.19.1）有数据流量的情况下，由阿里工程师人为地在杭州点的主IPSGW上down掉接口配置，造成主IPSecGW与杭州CE的IPSec连接中断。**

   **演练过程：**

1. **时间节点4/12 0.06，北京站点ping操作正常，延迟波动范围为42-46ms**

..

   |image2|\ **图2.1.1 数据连接图**

2. 时间节点4/12
   0.07,阿里工程师down掉上海节点主IPSec连接，数据流量中断，期间CE上bgp连接仍然存活。

..

   |image3|\ 图2.1.2 流量中断图

   |image4|\ 图2.1.3 bgp状态查看图

   3、时间节点4/12
   0.10（3min）,流量切换到备线进行传输，延迟波动范围为340-380ms，CE上BGP状态为connect

   |image5|\ 图2.1.4 流量恢复图

   |image6|\ 图2.1.5 bgp状态图

**测试结论：**\ 基本符合预期，仍有较大整改空间。

   **整改措施：PE与CE双方配置的bgp hold
   time时间为默认的180s，造成主备链路流程切换时间过长，预计调整CPE双方关于bgp
   hold
   time参数的配置，以期提高链路的情况检测的敏捷性，缩短链路切换的时间。见下期。**

 控制器故障，转发正常
---------------------

   **演练目的：验证当epoch中控与ce端的控制连接失效时，对数据流量的影响。**

   **预期结果：CE与epoch中控的控制连接，不会对CE的数据流量造成影响**

   **演练方式：在北京站点PC（10.0.20.101）与杭州点地wifi网关（10.0.19.1）有数据流量的情况下，由光联工程师人为地中断CE与epoch中控的控制连接，造成epoch中控无法登录管理CE设备。**

   **演练过程：**

1. **时间节点4/11 21.35
   北京ping测试正常，在CE上断掉与epoch的控制通道连接，epoch中空上无法连接到CE端。**

|image7|\ 图2.2.1 流量状况图

|image8|

图2.2.2 epoch中控图

2. 时间节点4/11 21.37
   恢复CE上与epoch中控的控制连接，过程中流量无影响，查看整个过程中的ping包无丢包情况。

..

   |image9|\ 图2.2.3 epoch中控图

   |image10|\ 图2.2.4 数据包接收统计图

   **测试结论：**\ 符合预期。

 CPE掉电 /硬件故障（瞬断）
--------------------------

   **演练目的：模拟CE设备短时间掉电的情况，以及对数据流量造成的影响。**

   **预期结果：CE掉电过程中，数据流量中断，CE上电短时间内，网络恢复。**

   **演练方式：在北京站点PC（10.0.20.101）与杭州点地wifi网关（10.0.19.1）有数据流量的情况下，由阿里工程师，手工拔插CE电源，形成CE短时间经历断电与上电的情况。**

   **演练过程：**

1. |image11|\ **时间节点4/11 22.11
   北京pc正常ping包，拔掉CE设备电源后一段时间插上**

..

   **图2.3.1 数据流量图**

2. **时间点4/11 22.13 预计IPSec重启时间为100s左右，数据重新连通。**

..

   |image12|\ **图2.3.2 流量中断图**

   |image13|\ 图2.3.3 bgp状态图

   |image14|\ 图2.3.4 数据流量恢复图

   **测试结论：符合预期。**

 CPE互联网故障(wan瞬断)
-----------------------

   **演练目的：验证当CE的互联网故障，造成主备IPSec连接中断的数据传输情况。**

   **预期结果：CE的wan网络链路中断的情况下，数据流量会一直中断，等到网络恢复，数据流量随即恢复。**

   **演练方式：在北京站点PC（10.0.20.101）与杭州点地wifi网关（10.0.19.1）有数据流量的情况下，由阿里工程师，拔插CE端的wan口线路，人为造成网络中断**

   **演练过程：**

1. **时间节点4/11 21.40 北京pc端ping流量测试正常**

..

   |image15|\ **图2.4.1 流量连通图**

2. |image16|\ 时间节点4/11 21.41
   短时间内拔插wan口线路，造成数据中断与恢复

..

   图2.4.2 流量恢复图

   **测试结论：符合预期**

 CPE LAN故障(lan)
-----------------

   **演练目的：验证当CE端，连接内网的端口down掉后，网络的数据传输情况**

   **预期结果：CE的Lan网络链路中断的情况下，连接此端口的pc数据流量会一直中断，等到网络恢复，数据流量随即恢复。**

   **演练方式：在北京站点PC（10.0.20.101）与杭州点地wifi网关（10.0.19.1）有数据流量的情况下，由光联工程师人为地down掉北京点的wifi接口，照成连接在此接口上的pc网络中断。**

   **演练过程：**

1. |image17|\ **时间节点4/11 21.43
   北京站点ping测试正常，短时间内在ce上down/up
   wifi接口,造成lan口中断。**

..

   **图2.5.1 数据连通图**

2. 时间节点 4/11
   21.45恢复WiFi接口up状态，持续一段时间未恢复连接，pc端wifi已从其他ap获取IP，重新连接CE端WiFi（21.48）即可正常通信。

..

   |image18|\ 图2.5.2 数据中断图

   |image19|\ 图2.5.3 数据恢复图

   **测试结论：**\ 符合预期

 CPE流量过载
------------

   **演练目的：验证当组网链路上存在大量数据时，对正常业务数据的影响情况**

   **预期结果：当网络内出现大量数据时，数据流量整体时延会出现可接受范围的上升。**

   **演练方式：在北京站点PC（10.0.20.101）与杭州点地wifi网关（10.0.19.1）有数据流量的情况下，由光联工程师与阿里云工程师，联合使用iperf3工具，形成大量流量数据传输，查看大流量数据形成后对原本的ping数据的整体时延影响情况。**

   **演练过程：**

1. **时间节点4/11 22.29
   北京pc作为iperf3的服务端，杭州CE作为iperf3客户端，向北京pc发起udp流量。北京pc出现丢包和乱序的情况**

..

   |image20|\ |image21|\ 图2.6.1 iperf3发送端状况图

   **图2.6.2 iperf3 接收端状况图**

   **测试结论：不符合预期**

   **整改措施：演练过程中出现报文丢失，失序的情况，通过初步排查，判定数据传输过程中，本地带宽导致的丢包失序情况，接下来可以搭建新的实验场景，对流量满载情况的，流量传输质量进行再次测试。**

故障演练（2022/4/13）
=====================

.. _vpngw故障主ipsecgw中断-1:

VPNGW故障——主IPSecgw中断 
-------------------------

   **演练目的：验证当主IPSec连接故障，流量切换情况**

   **预期结果：流量默认走主线路，当主线路down掉后，流量会切换到备线，通过配置BGP的hold-time时间为15秒，预计断线与切换的时间会持续15s。**

   **演练方式：在北京站点的pc（10.0.20.101）向杭州站点CE（10.0.19.1）进行ping包测试，过程中，down掉杭州站点主线（上海线）的IPSec连接，观测流量数据切换情况.**

   **数据观测记录：（下述时间节点并非真实时间，重在探测时间间隙）**

   **通过在北京点ce上对端口进行抓包，实时查看接口的数据流量转发情况。**

**图一数据显示，由10.0.20.1<——>10.0.19.1的数据流量正常转发。截至时间点：14：10：00，之后进行上海线路中断。**

**图二数据显示，由14：10：00至14：10：25这段时间，北京的主IPSec虚拟接口上都未有10.0.20.1<——>10.0.19.1的流量，至14：10：25之后开始出现连续的实验数据流量，整个过程共持续15s，符合实验预期。**

**测试结论：基本符合预期，仍有整改空间**

**整改措施：当主线恢复后，流量由备线切换回主线，经实验测试结果来说，大概一分种。需要根据实际情况调整配置参数，缩短由备线回切向主线的时间。**

   **数据截图：**

|image22|

图3.1.1 ping流量来回正常

|image23|

图3.1.2 流量切换时间图

|image24|

图3.1.3 流量稳定情况图

 CPE互联网故障(wan)
-------------------

   **演练目的：验证当CE的互联网故障，造成主备IPSec连接中断及恢复的数据传输情况。**

   **预期结果：CE的wan网络链路中断的情况下，数据流量会一直中断，等到网络恢复，数据流量经过一段时间恢复。**

**演练方式：在杭州站点PC（10.0.19.128）与北京点的wifi网关（10.0.20.1）有数据流量的情况下，由阿里工程师，拔插CE端的wan口线路，过程持续5分钟，人为造成网络中断。**

**数据观测记录：（下述时间节点并非具体时间，重在观测时间间隙）**

**时间记录：杭州pc与北京CE数据传输正常，12:12，拔掉wan口，模拟网络中断，过程持续5分钟。**

**插回wan口网线时间为17：06**

**互联网（114.114.114.114）通的时间为17：19**

**中控连接时间点19：54**

**BGP状态切换为establish时间为，22.06/27**

**Ping流量测试连通时间22.40**

**综上，拔掉网线至数据流量恢复的时间共持续5m36s，其中，互联网连接上的时间为13s，中控连接时间为2m48s.**

**测试结论：不符合预期**

**整改措施：ipsec的IPSec
sa存活性和重连机制在当前配置下错误，同时IPSec协议本身在此不够完善。使用新配置可以缓解此处问题，后续将发布补丁从根本上解决。**

|image25|

图3.2.1 杭州pc与北京CE数据传输正常

|image26|

图3.2.2 杭州ce上查看BGP connect状态持续5分钟。

|image27|\ 图3.2.3 IPSec存活时间查看为22m51s

CPE掉电 /硬件故障
-----------------

   **演练目的：模拟CE设备长时间掉电之后数据连接的恢复情况。**

   **预期结果：CE掉电过程中，数据流量中断，CE上电经过一段时间后，私网互访正常。**

   **演练方式：在杭州站点PC（10.0.19.128）与北京点地wifi网关（10.0.20.1）有数据流量的情况下，由阿里工程师，手工拔插CE电源（过程持续3分钟），形成CE经历持续一段时间的断电与上电的情况。**

**数据观测记录：（下述时间节点并非具体时间，重在观测时间间隙）**

32：30，由阿里工程师拔掉杭州ce站点电源。

35：30，阿里工程师插回杭州ce站点电源

37：00，公网通（114.114.114.114），同期IPSec sa连接成功

37：37，控制平面连接成功

39：13 pc与CE之间的数据连接通路

综上：从上电到数据连通时间共持续4m13s，其中从启动到连接公网花费1m30s，连接公网之后接上控制平面花费37s，连接公网之后数据连通花费2m13s。

**测试结论：**\ 基本符合预期，仍有较大改进空间

**整改措施：**\ 在IPSec sa
up知道VPN胡同存在较大的时延，需要与阿里工程师再次场景复现，并协同诊断，后续发布补丁解决问题。

|image28|

图3.3.1 杭州pc与北京CE数据连接成功

|image29|\ 图3.3.2 BGP连接成功

|image30|

图3.3.3 于38：15查看IPSec以启动1m6s，预计启动时间为37：07

 模拟打流测试
-------------

   **演练目的：组网链路能否在100M的数据传输情况下，进行数据的稳定传输**

   **预期结果：当网络内出现100M/s的数据时，数据基本能够稳定传输，丢包情况在可接收范围内。**

   **演练方式：两台ce分别连接到两台pe，ce之间使用iperf工具进行大流量模拟**

   **数据观测记录：**\ 100M的测试环境的打流，模拟北京到杭州的测试环境，打流100M
   实际能够达到到91M。（考虑本地链路的传输情况，基本符合预期）

|image31|

图3.4.1 iperf使用tcp进行大流量模拟

|image32|

图3.4.2 iperf使用udp进行大流量模拟

**测试结论：**\ 符合预期

.. |image0| image:: ./imgs//media/image6.png
   :width: 6.26736in
   :height: 4.02222in
.. |image1| image:: ./imgs//media/image7.png
   :width: 6.68472in
   :height: 1.48681in
.. |image2| image:: ./imgs//media/image8.png
   :width: 5.59097in
   :height: 1.875in
.. |image3| image:: ./imgs//media/image9.png
   :width: 5.61111in
   :height: 1.70764in
.. |image4| image:: ./imgs//media/image10.png
   :width: 5.54514in
   :height: 0.84444in
.. |image5| image:: ./imgs//media/image11.png
   :width: 5.18056in
   :height: 1.75in
.. |image6| image:: ./imgs//media/image12.png
   :width: 4.97778in
   :height: 0.80556in
.. |image7| image:: ./imgs//media/image13.png
   :width: 4.77778in
   :height: 1.88681in
.. |image8| image:: ./imgs//media/image14.png
   :width: 4.84931in
   :height: 2.04861in
.. |image9| image:: ./imgs//media/image15.png
   :width: 5.46458in
   :height: 2.21667in
.. |image10| image:: ./imgs//media/image16.png
   :width: 5.51319in
   :height: 1.72639in
.. |image11| image:: ./imgs//media/image17.png
   :width: 5.89444in
   :height: 2.65347in
.. |image12| image:: ./imgs//media/image18.png
   :width: 5.91042in
   :height: 0.93958in
.. |image13| image:: ./imgs//media/image19.png
   :width: 5.86042in
   :height: 1.18056in
.. |image14| image:: ./imgs//media/image20.png
   :width: 5.28403in
   :height: 1.73333in
.. |image15| image:: ./imgs//media/image21.png
   :width: 5.33958in
   :height: 1.62639in
.. |image16| image:: ./imgs//media/image22.png
   :width: 5.31042in
   :height: 2.47153in
.. |image17| image:: ./imgs//media/image23.png
   :width: 5.42361in
   :height: 1.67153in
.. |image18| image:: ./imgs//media/image24.png
   :width: 5.40278in
   :height: 3.24375in
.. |image19| image:: ./imgs//media/image25.png
   :width: 5.43056in
   :height: 1.78472in
.. |image20| image:: ./imgs//media/image26.png
   :width: 5.83264in
   :height: 2.66319in
.. |image21| image:: ./imgs//media/image27.png
   :width: 5.8875in
   :height: 3.80278in
.. |image22| image:: ./imgs//media/image28.png
   :width: 5.90972in
   :height: 1.98611in
.. |image23| image:: ./imgs//media/image29.png
   :width: 6.48472in
   :height: 5.72153in
.. |image24| image:: ./imgs//media/image30.png
   :width: 6.05208in
   :height: 2.52083in
.. |image25| image:: ./imgs//media/image31.png
   :width: 6.03819in
   :height: 2.22569in
.. |image26| image:: ./imgs//media/image32.png
   :width: 6.29514in
   :height: 1.4875in
.. |image27| image:: ./imgs//media/image33.png
   :width: 6.38333in
   :height: 4.18958in
.. |image28| image:: ./imgs//media/image34.png
   :width: 6.04931in
   :height: 1.66389in
.. |image29| image:: ./imgs//media/image35.png
   :width: 6.03681in
   :height: 1.57083in
.. |image30| image:: ./imgs//media/image36.png
   :width: 6.03542in
   :height: 3.53889in
.. |image31| image:: ./imgs//media/image37.png
   :width: 5.20833in
   :height: 3.09514in
.. |image32| image:: ./imgs//media/image38.png
   :width: 5.125in
   :height: 2.60417in
